=== Statistics <table_statistics>

Statistics are not only embedded in data files (see @file_statistics), but also in table metadata. Statistics embedded in data files have the disadvantage that the query planner does not have access to this information because the Parquet files have not yet been retrieved. However, the query planner has access to the table metadata and can use it to perform data skipping early in the query planning process.

Table formats handle statistics in different ways: Hive and Hudi use supplementary data structures to store them, and Iceberg and Delta Lake store them along with the metadata.

==== Hive

Hive tables store their statistics in the Hive Metastore (HMS). We will skip the details because statistics generation is a bit confusing: Hive as an execution engine has gone through several milestones, first not supporting statistics at all, then adding table and partition statistics in Hive 0.7.0, and finally adding column-level statistics in 0.10.0 @ApacheHiveStatsDev. For some, but not all versions, these statistics are collected on write.

Trino collects column-level statistics differently than Spark: Spark puts statistics into the `TABLE_PARAMS` table, while Trino puts them into the `TAB_COL_STATS` table in the HMS. In Trino, statistics are generated by default (`hive.collect-column-statistics-on-write=true`) while in Spark they have to generated manually using the `ANALYZE TABLE` syntax.

==== Hudi

Hudi will only automatically collect statistics on write when explicitly enabled by setting `hoodie.metadata.index.column.stats.enable` @HudiRFC27. This feature requires the use of the Hudi metadata table, which is enabled by default (see @metadata_concepts). The table statistics are stored in the `column_stats` partition of the Hudi metadata table (in `.hoodie/metadata/column_stats`).

The use of these statistics for data skipping must also be explicitly enabled by setting `hoodie.enable.data.skipping`.

==== Iceberg

Each manifest file contains `column_sizes`, `value_counts`, `null_value_counts`, `nan_value_counts`, `lower_bounds`, and `upper_bounds` for each column per file. These statistics are automatically generated on write in both Spark and Trino.

There is also a standard way to store additional statistics not covered by the above properties.

In addition, Trino generates extended statistics either on write or manually with `ANALYZE table_name;`. Extended statistics are stored in files with the extension `.stats` in the `metadata` directory. These files are in the Puffin file format, for which there is currently no CLI reader available. According to the Puffin specification @IcebergPuffinSpec, the file can contain multiple blob types, the only one specified being a data sketch produced by the Apache DataSketches library, which contains an approximate number of distinct values. These `.stats` files are added to an existing snapshot as a new table metadata file, adding only entries to the `statistics` field array. The automatic generation of extended statistics in Iceberg can be disabled by unsetting `iceberg.extended-statistics.collect-on-write`. Manual generation via `ANALYZE` can be disabled by unsetting `iceberg.extended-statistics.enabled`.

While this mechanism is part of the standard, there is currently no support for it in Spark.

==== Delta Lake

`add` actions in a Delta file contain `size` and optionally `stats` for each data file: the statistics include `numRecords` for the entire table, and `minValues`, `maxValues`, and `nullCount` for each column (see _Per-file Statistics_ in @DeltaTransactionLog). The `remove` action may also contain statistics, but it does not carry `size` because no data file is added. This behavior is the same for both Spark and Trino.

Trino also stores extended statistics in `_delta_log/_trino_meta/extended_stats.json`, including `totalSizeInBytes` for some columns and `ndvSummary` for all columns. These are automatically created as part of the regular write process. This behavior can be disabled by unsetting the `delta.extended-statistics.collect-on-write` catalog parameter. Extended statistics are custom to Trino and are not part of the standard. Unlike extended statistics in Iceberg, these statistics refer to the latest snapshot and do not apply to previous snapshots.

==== Conclusions

Data skipping is not the only benefit of collecting table statistics. Modern query engines use cost-based optimizers (CBO) rather than rule-based optimizers (RBO) to determine the ideal query plan by computing multiple query plans and selecting the one with the lowest cost (e.g., a combination of CPU, memory, disk, and network). Several statistics are used to estimate the cost of each plan, such as the number of rows in a table, the distribution of the data, the number of distinct values, the number of `NULL` values, and so on.

These metrics must accurately describe the data, otherwise the optimizer may make the wrong decision. For this reason, statistics-based CBOs are controversial. Spark has its CBO disabled by default (it can be enabled by setting `spark.sql.cbo.enabled`), but the Spark-based Databricks Runtime (DBT) platform has it enabled @DatabricksCBO and so does Trino for all of its connectors (`hive.table-statistics-enabled=true`, and so on).

Given the requirement imposed by object stores that data files cannot change, the use of statistics-based CBOs should be reasonably safe as long as statistics generation is a) automated, not manual, and b) supported in the same way by all execution engines participating on the same storage system. These two criteria should prevent missing or outdated statistics. Since lakehouse systems compute statistics once at write time, even expensive operations (such as creating data sketches) can be justified because they only need to be performed once.
