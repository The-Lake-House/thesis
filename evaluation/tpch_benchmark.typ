#import "/flex-caption.typ" as flex

== TPC-H Benchmark <tpch_benchmark>

In this first comparison, we run TPC-H query 1 (see @tpch_query1) on tables in four different formats with different scale factors to learn about the general characteristics of the table formats, storage formats, and query engines.

Query 1 (see @tpch_query1) reports the amount of business that was billed, shipped, and returned by a fictious wholesale supplier @TPCH. This query uses data from only one table, the `lineitem` table.

#figure(
```sql
select
    l_returnflag,
    l_linestatus,
    sum(l_quantity) as sum_qty,
    sum(l_extendedprice) as sum_base_price,
    sum(l_extendedprice*(1-l_discount)) as sum_disc_price,
    sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,
    avg(l_quantity) as avg_qty, avg(l_extendedprice) as avg_price,
    avg(l_discount) as avg_disc,
    count(*) as count_order
from lineitem
where l_shipdate <= date '1998-12-01' - interval '90' day
group by l_returnflag, l_linestatus
order by l_returnflag, l_linestatus;
```,
caption: [TPC-H Q1],
) <tpch_query1>

In the `lineitem` table generated by the `tpch` connector in Trino, the `l_shipdate` has the range `[1992-01-02, 1998-12-01]` as specified. Subtracting 90 days from this range does not significantly reduce the table size, i.e., the query represents an almost complete table scan. `l_returnflag` has three distinct values: `A`, `R`, `N`. `A` and `R` are evenly distributed, and `N` is about twice as common as the others. `l_linestatus` has two distinct values: `O` and `F`. They are equally distributed.

These initial tests were mainly done in Trino (using Spark to create Hudi tables) using default options on the test system. We ran several rounds of this benchmark: a smaller one with a scale factor of 2 and a larger one with a scale factor of 100.

=== Scale Factor 2

First, we created a MinIO bucket named `tpch`:

```bash
mcli mb minio/tpch
```

We then created a schema for each table format that stores data in a "subdirectory" of the `tpch` bucket:

```sql
CREATE SCHEMA hive.tpch_hive WITH (location = 's3a://tpch/hive');
CREATE SCHEMA iceberg.tpch_iceberg WITH (location = 's3a://tpch/iceberg');
CREATE SCHEMA delta.tpch_delta WITH (location = 's3a://tpch/delta');
```

The Hudi schema must be created in Spark because Trino only has read support for Hudi. Note the slightly different syntax:

```sql
CREATE SCHEMA tpch_hudi LOCATION 's3a://tpch/hudi';
```

In this case, the location is associated with a schema. If set, tables created within the schema are created in the specified location, otherwise they are created in the warehouse directory (see @setup).

We created the `lineitem` tables with a scale factor of 2 using a `CREATE TABLE AS SELECT` (CTAS) through the `tpch` connector:

```sql
CREATE TABLE hive.tpch_hive.lineitem AS SELECT * FROM tpch.sf2.lineitem;
CREATE TABLE iceberg.tpch_iceberg.lineitem AS SELECT * FROM tpch.sf2.lineitem;
CREATE TABLE delta.tpch_delta.lineitem AS SELECT * FROM tpch.sf2.lineitem;
```

The Hudi table must be created in Spark, for the same reason, again using a slightly different syntax. The existing Hive table can be used as the data source.

```sql
CREATE TABLE tpch_hudi.lineitem USING hudi AS SELECT * FROM tpch_hive.lineitem;
```

We checked that each table had the same number of records: `SELECT COUNT(*) FROM lineitem;` returned 11,997,996 records for each table. We also verified that the same `(orderkey, linenumber)` tuples exist in all tables, e.g., using Hive as a reference:

```bash
diff <($TRINO_HOME/bin/trino --catalog=hive --schema=tpch_hive --execute="SELECT orderkey, linenumber FROM lineitem;" | sort) <($TRINO_HOME/bin/trino --catalog="$FORMAT" --schema="tpch_$FORMAT" --execute="SELECT orderkey, linenumber FROM lineitem;" | sort)
```

Finally, we performed Q1 (see @tpch_query1) 30 times each with a 5 round warm-up as described in @benchmarks.

```bash
NUM_WARMUP=5
NUM_REPS=30

echo 'format,rep,time' > timings
for FORMAT in hive hudi iceberg delta; do
    # Warmup
    for REP in $(seq 1 "$NUM_WARMUP"); do
        $TRINO_HOME/bin/trino --catalog="$FORMAT" --schema="tpch_$FORMAT" --file=q1.sql --output-format=NULL
    done
    # Benchmark
    for REP in $(seq 1 "$NUM_REPS"); do
        QUERY_ID="${FORMAT}_${REP}_$(openssl rand -hex 4)"
        mcli admin trace minio > "${FORMAT}_${REP}.trace" 2> /dev/null &
        TRACE_PID=$!
        sar -o "${FORMAT}_${REP}.sar" -A 1 &> /dev/null &
        SAR_PID=$!
        $TRINO_HOME/bin/trino --source="$QUERY_ID" --catalog="$FORMAT" --schema="tpch_$FORMAT" --file=q1.sql --output-format=NULL
        kill -SIGINT "$SAR_PID"
        kill -SIGINT "$TRACE_PID"
        $TRINO_HOME/bin/trino --catalog=system --schema=runtime --execute="SELECT '$FORMAT' AS format, '$REP' AS rep, TO_MILLISECONDS(\"end\" - started) AS time FROM queries WHERE source = '$QUERY_ID';" --output-format=CSV_UNQUOTED >> timings 2> /dev/null
    done
done
```

The timings were analyzed using the R programming language @rcoreteamLanguageEnvironmentStatistical2023 with the `tidyverse` packages @wickhamWelcomeTidyverse2019:

```R
library(readr)
library(dplyr)

read_csv("timings") %>%
    group_by(format) %>%
    summarize(n = n(), mean = mean(time), sd = sd(time))
```

#figure(
  table(
    columns: 4,
    [*Table Format*], [*N*], [*Mean Time [ms]*], [*Standard Deviation [ms]*],
    [Hive], [30], [1757], [17.2],
    [Hudi], [30], [1192], [11.1],
    [Iceberg], [30], [1134], [27.7],
    [Delta Lake], [30], [1254], [11.7],
  ),
  caption: flex.flex-caption(
    [Initial benchmark results of TPC-H query 1 with a scale factor of 2],
    [TPC-H Q1 SF=2: Initial results],
  ),
) <tpch_query1_sf2_initial_results>

As shown in @tpch_query1_sf2_initial_results, the timings between Hudi, Iceberg, and Delta Lake are similar, but Hive performs worse than its competitors. However, this benchmark, while not immediately obvious, has a few problems that make the table formats difficult to compare.

*Number of files*: The number of files varies greatly.

The following are abridged file listings for each table format, omitting the timestamp and storage class from the output of `mcli ls -r`. For more information on the directory structure of each table format see @metadata_management.

For Hive:

```bash
$ mcli ls -r minio/tpch/hive/lineitem
 37MiB 20240107_110316_00003_99ev7_00787ddf-3f94-471e-acd7-7de60f850b3e
3.7MiB 20240107_110316_00003_99ev7_234a8723-2caf-419f-a8d6-4e7fbba5cb51
9.7MiB 20240107_110316_00003_99ev7_307fba4a-205a-4518-bd62-edcca3727e0a
 21MiB 20240107_110316_00003_99ev7_3c8e4c57-4ed3-41b6-a2ba-22e216b8e4fe
7.5MiB 20240107_110316_00003_99ev7_3e7992f6-94f5-4acd-8001-4c70bf762f5f
 25MiB 20240107_110316_00003_99ev7_4625e4b4-27eb-48bb-af15-61ec0ec22e48
 17MiB 20240107_110316_00003_99ev7_51648306-7199-486a-8615-016d379ed37b
 67MiB 20240107_110316_00003_99ev7_5271e8df-3b39-4e64-8505-c54f644e038f
5.5MiB 20240107_110316_00003_99ev7_60b4bdec-45fe-4fe9-96a1-e6ba5ce55382
 47MiB 20240107_110316_00003_99ev7_8ed8abe7-d3a7-4dbf-88b6-4d57e6764807
704KiB 20240107_110316_00003_99ev7_9c03de17-a7ed-48f5-b21c-cb1c01ad3076
 30MiB 20240107_110316_00003_99ev7_bc79f8c9-3142-4c93-afc4-629d8d93e811
2.2MiB 20240107_110316_00003_99ev7_d4455eb2-4521-4acd-8213-10e47d27d8c8
 15MiB 20240107_110316_00003_99ev7_f6780be4-d098-4d5e-b646-9661fa454fd6
 12MiB 20240107_110316_00003_99ev7_ffc79850-9097-4ef8-8884-819c1a216717
```

For Hudi:

```bash
$ mcli ls -r minio/tpch/hudi/lineitem
9.5KiB STANDARD lineitem/.hoodie/20240107120531841.commit
    0B STANDARD lineitem/.hoodie/20240107120531841.commit.requested
    0B STANDARD lineitem/.hoodie/20240107120531841.inflight
1.5KiB STANDARD lineitem/.hoodie/hoodie.properties
8.3KiB STANDARD lineitem/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
    0B STANDARD lineitem/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
    0B STANDARD lineitem/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.requested
8.6KiB STANDARD lineitem/.hoodie/metadata/.hoodie/20240107120531841.deltacommit
1.5KiB STANDARD lineitem/.hoodie/metadata/.hoodie/20240107120531841.deltacommit.inflight
    0B STANDARD lineitem/.hoodie/metadata/.hoodie/20240107120531841.deltacommit.requested
  676B STANDARD lineitem/.hoodie/metadata/.hoodie/hoodie.properties
   80B STANDARD lineitem/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.1_0-0-0
 13KiB STANDARD lineitem/.hoodie/metadata/files/.files-0000-0_00000000000000010.log.2_0-14-21
   96B STANDARD lineitem/.hoodie/metadata/files/.hoodie_partition_metadata
6.6KiB STANDARD lineitem/.hoodie/metadata/files/files-0000-0_0-6-5_00000000000000010.hfile
   96B STANDARD lineitem/.hoodie_partition_metadata
 46MiB STANDARD lineitem/1104cc65-109a-4ad4-ad93-9eceb067b1b9-0_6-14-0_20240107120531841.parquet
 71MiB STANDARD lineitem/15aa9727-eaeb-43e2-9627-8566c5ee6efd-0_0-8-0_20240107120531841.parquet
 30MiB STANDARD lineitem/16bb95ac-15f9-4a98-9078-a7b589f43400-0_4-12-0_20240107120531841.parquet
 44MiB STANDARD lineitem/2496801a-7b49-4d98-9460-c3553f74afb7-0_2-10-0_20240107120531841.parquet
8.9MiB STANDARD lineitem/6ca47486-f132-433b-81a1-e4e93c6e532b-0_5-13-0_20240107120531841.parquet
 24MiB STANDARD lineitem/a36c5af7-508e-4791-a4f6-1ff4083b46e8-0_8-16-0_20240107120531841.parquet
 36MiB STANDARD lineitem/c9f25d26-a57d-49e5-b5e3-358b8a48767e-0_3-11-0_20240107120531841.parquet
 56MiB STANDARD lineitem/dd1b98d8-201a-4e3d-aaef-933ee2a87b98-0_1-9-0_20240107120531841.parquet
 44MiB STANDARD lineitem/f6300db9-8799-41e2-b1af-fef5629ceda5-0_7-15-0_20240107120531841.parquet
```

For Iceberg:

```bash
$ mcli ls -r minio/tpch/iceberg/lineitem-5341fb42689a4ee59f4e57f1aca94e86
5.6MiB data/20240107_110333_00006_99ev7-06ef2859-cb90-4e06-abdd-961e91cb0d07.parquet
 66MiB data/20240107_110333_00006_99ev7-1719834e-48ba-4a4b-a7af-fe7ac1538bdc.parquet
 21MiB data/20240107_110333_00006_99ev7-2630527d-c12f-41f0-a2ab-c7b00f65cd06.parquet
 30MiB data/20240107_110333_00006_99ev7-3af3cf31-21a0-44cd-8a5e-7dba6d09c98b.parquet
946KiB data/20240107_110333_00006_99ev7-3b4071d1-21d6-457c-9e27-60c46544c526.parquet
3.9MiB data/20240107_110333_00006_99ev7-4ff9f4c5-da2d-4a2e-b413-2fa47ce830f4.parquet
 17MiB data/20240107_110333_00006_99ev7-5674ab7c-6d2e-45ba-b67a-327b82c50bec.parquet
 15MiB data/20240107_110333_00006_99ev7-5aaabe47-2881-4105-bec7-1dba440a20b6.parquet
2.6MiB data/20240107_110333_00006_99ev7-7de41ae3-40e7-4a83-b4ee-2482ac278b4a.parquet
 46MiB data/20240107_110333_00006_99ev7-965213f7-f4a7-4f2e-8795-1b7c996b648e.parquet
 36MiB data/20240107_110333_00006_99ev7-e47d7157-ee5d-404a-8197-5da88eff128e.parquet
9.8MiB data/20240107_110333_00006_99ev7-e5581bff-7fe6-4190-9def-7c0a714c515a.parquet
 12MiB data/20240107_110333_00006_99ev7-f5979269-3925-46ae-9b82-36e6c154ba0b.parquet
 25MiB data/20240107_110333_00006_99ev7-f6e07c3c-e734-498d-a6a1-a1184bf80389.parquet
7.6MiB data/20240107_110333_00006_99ev7-f8485101-7cef-4c4f-a2e6-9ed560dab7e0.parquet
3.4KiB metadata/00000-6bf3638e-97af-4b01-b58e-36742830db75.metadata.json
7.1KiB metadata/00001-fb2ff48b-6bdb-45ad-a1ff-bb2c958858fe.metadata.json
215KiB metadata/20240107_110333_00006_99ev7-b38f8d33-b6bb-4624-ad24-171a9f708748.stats
 10KiB metadata/b200a617-6163-413c-9a1a-f41a8a517eac-m0.avro
4.2KiB metadata/snap-7861196536919036658-1-b200a617-6163-413c-9a1a-f41a8a517eac.avro
```

For Delta Lake:

```bash
$ mcli ls -r minio/tpch/delta/lineitem-b3cce8e6785f483fa8ca9ee0333444f0
 33MiB 20240107_110347_00009_99ev7_0091034d-2bad-4bee-ab42-a02ef2005be6
1.5MiB 20240107_110347_00009_99ev7_1f1aafcf-58b2-42c8-a156-ea70457e9b9b
 23MiB 20240107_110347_00009_99ev7_3d37b486-3c18-4ed2-9299-609c6c4079ce
3.4MiB 20240107_110347_00009_99ev7_583b8218-15c9-40f8-927c-ce5cad3e629b
 16MiB 20240107_110347_00009_99ev7_58f38c99-5129-416e-bfb1-79e435b9dd15
5.2MiB 20240107_110347_00009_99ev7_7fdcdec3-60a9-4acb-b796-da6aaae76aac
 49MiB 20240107_110347_00009_99ev7_830f98e5-ca0c-4e0d-a9b7-296bf207375c
 28MiB 20240107_110347_00009_99ev7_9e061904-51a8-4680-98f9-b638311a6a83
 20MiB 20240107_110347_00009_99ev7_a76c3fce-e516-4d48-8f26-d5a16b3a24ba
 62MiB 20240107_110347_00009_99ev7_b1123cb6-990f-48f7-94d1-3569a04b4f39
 88MiB 20240107_110347_00009_99ev7_caab2290-e698-40f4-b4ab-3f602b1cf12e
7.6MiB 20240107_110347_00009_99ev7_ea2caec9-745d-4f74-8fd7-8fc42af34c22
 10MiB 20240107_110347_00009_99ev7_edc32a4a-ef41-439e-9b7a-01cb3bda3b2c
 13MiB 20240107_110347_00009_99ev7_ee85b74e-faf5-4d0b-b9b9-2adf792081f4
 40MiB 20240107_110347_00009_99ev7_f8ae5314-e884-4b50-bd9f-603d9f886c7d
 20KiB _delta_log/00000000000000000000.json
 23KiB _delta_log/_trino_meta/extended_stats.json
```

Hudi automatically creates the Hudi metadata table (see @metadata_management), and both Iceberg and Delta Lake use extended statistics (see @table_statistics), which in the case of Iceberg results in two table metadata files for this single insert.

Spark and Trino produce one Parquet file per task and these files can get small on a modern multi-core system, which leads to the small files problem as described in @file_sizes. Hive, Iceberg and Delta Lake tables contain 15 data files, Hudi tables only 9.

In Trino, there are two ways to address this problem:

- Run the `optimize` procedure (see @file_merging) after the insert. This option has the disadvantage that for Iceberg and Delta Lake new snapshots are created and old files are left behind, creating an unfair advantage for Hive, which does not have deal with versioning. To some extent, this can be mitigated by table cleanups (see @cleanup).

- Limit the number of concurrent writer threads per task per node. This can be achieved using the `task_max_writer_count` session property or the `task.max-writer-count` catalog property in Trino >= 428. If set to `1`, there will be only a single writer and thus only a single file (unless the file is split because the maximum file size is reached). Note that this setting will massively degrade throughput and will only work if a single node is used.

Spark can be told to create a single file by using the `/*+ COALESCE(1) */` hint immediately after the `SELECT` keyword:

```sql
CREATE TABLE tpch_hudi.lineitem USING hudi AS SELECT /*+ COALESCE(1) */ * FROM tpch_hive.lineitem;
```

Hints were introduced in Spark 2.4 @SPARK24940CoalesceRepartition. Importantly, hints are distinguished from regular comments by the presence of a `+` symbol immediately after the start of the comment @ApacheSparkGitHub.

Unfortunately, this construct is only supported for `SELECT` statements, so it will not work for `INSERT`s that explicitly specify their values instead of retrieving them via a `SELECT` statement. A workaround is to change the `INSERT` from `INSERT INTO database.table VALUES (value1), (value2), (value3);` to `INSERT INTO database.table SELECT /*+ COALESCE(1) */ * FROM (VALUES (value1), (value2), (value3));`.

There is also a `REPARTITION` hint that works similarly to `COALESCE`, but requires a shuffle step that becomes prohibitive in terms of memory and disk space for large datasets @ApacheSparkGitHuba. Note that the term _partition_ in this case refers to a portion of the internal representation of the dataset in Spark (RDD) that is distributed to the writers, not partitions in the sense of data layout partitioning (see @data_layout).

*Different record order*: The `tpch` connector in Trino, while deterministic, produces the result set in parallel and the order of the table may differ from query to query. While tuples in the relational model are not ordered at all, from a comparison point of view, a different order may result in slightly different row group sizes, compression ratios, etc. Therefore, we set `tpch.splits-per-node=1` in the `tpch` catalog configuration (which otherwise defaults to the number of available processors @TrinodbTrinoGitHub) to generate only one split per node and thus have sequential processing that guarantees the order.

*Type of files*: Hive creates ORC files, all other table formats create Parquet files. This is not immediately obvious because Hive omits the file extension, but when probed with the `parquet-cli` tool, an error occurs:

```bash
$ parquet-cli meta 20240107_110316_00003_99ev7_00787ddf-3f94-471e-acd7-7de60f850b3e
Unknown error
java.lang.RuntimeException: file:/home/agrueneberg/tpch/hive/lineitem/20240107_110316_00003_99ev7_00787ddf-3f94-471e-acd7-7de60f850b3e is not a Parquet file. Expected magic number at tail, but found [79, 82, 67, 25]
```

As described in @supported_storage_formats, the file format in Hive can be changed to Parquet either by setting the catalog configuration property `hive.storage-format=PARQUET` or by adding `WITH (format = 'PARQUET')` to the `CREATE TABLE` statement.

*Compression codec*: This is also not immediately obvious, but when probed with `parquet-cli meta`, the data files generated by Hudi, Iceberg, and Delta Lake show a different codec: Hudi uses gzip (`G` in `encodings`), Iceberg uses Zstd (`Z` in `encodings`), Delta Lake uses Snappy (`S` in `encodings`).

As described in @supported_compression_codecs, the same compression codec can be used by setting the `compression_codec` session property:

```sql
SET SESSION hive.compression_codec = 'GZIP';
SET SESSION iceberg.compression_codec = 'GZIP';
SET SESSION delta.compression_codec = 'GZIP';
```

In Spark, the following option must be passed: `--conf spark.sql.parquet.compression.codec=GZIP`, which is already the default for Hudi.

*Size of files*: Related to the choice of storage format and compression codec is the size of the files: the data files in the Hive table take up 300M, in Hudi 354M, in Iceberg 296M, and in Delta Lake 399M. The size of the individual data files is also unbalanced: some are much smaller than others, resulting in a wide range from 704 KiB to 88 MiB.

*Different split sizes*: Each connector uses a different split size (see @split_generation). In @split_size_determination, we determined that the ideal split size for Parquet files is 118 MiB. This value can be used for all table formats except Hudi, which does not allow the split size to be set and uses the file length by default.

*Other minor issues*:

- Iceberg and Delta Lake add a unique string to the table locations. This is confusing when trying to list the files with `mcli`. This can be disabled by setting `iceberg.unique-table-location=false` and `delta.unique-table-location=false` in the respective catalog configuration.

- Table formats based on `ListObjectsV2` are potentially penalized by data from other table formats stored in the bucket. We decided to stick with the single bucket approach because there are not many files involved, and a single bucket makes cleanup easier.


=== Scale Factor 2 (Revised)

We ran another round of the benchmark, addressing most of the issues highlighted in the previous section addressed:

```sql
-- Use the same compression codec for all storage files
SET SESSION hive.compression_codec = 'GZIP';
SET SESSION iceberg.compression_codec = 'GZIP';
SET SESSION delta.compression_codec = 'GZIP';

-- Produce fewer files
SET SESSION task_max_writer_count = 1;

-- Do not collect extended statistics on write
SET SESSION iceberg.collect_extended_statistics_on_write = false;
SET SESSION delta.extended_statistics_collect_on_write = false;

CREATE TABLE hive.tpch_hive.lineitem WITH (format = 'PARQUET') AS SELECT * FROM tpch.sf2.lineitem;
CREATE TABLE iceberg.tpch_iceberg.lineitem AS SELECT * FROM tpch.sf2.lineitem;
CREATE TABLE delta.tpch_delta.lineitem AS SELECT * FROM tpch.sf2.lineitem;
```

Each table format places an upper limit on the size of data files, as described in @file_sizes. Hudi produces smaller data files than other table formats, so the maximum Parquet file size must be increased to the same default as for other table formats (1 GiB):

```sql
CREATE TABLE tpch_hudi.lineitem
USING hudi
TBLPROPERTIES (
  'hoodie.metadata.enable' = false,
  'hoodie.parquet.max.file.size' = 1073741824
)
AS SELECT /*+ COALESCE(1) */ * FROM tpch_hive.lineitem;
```

After creating the tables, listing the files in the `tpch` bucket now shows a reduction in the number of files, as well as an increase in the average size of the data files. Importantly, all Parquet files are now similar in size and use the same compression codec. Only minor differences in metadata fields and schema specification remain. Hudi is an exception due to the added metadata (see @supported_storage_formats).

For Hive:

```bash
$ mcli ls -r minio/tpch/hive/lineitem
284MiB 20231230_134731_00018_9tjiz_7ff34205-31f3-4202-973c-6397ccec9ac9
```

For Hudi:

```bash
$ mcli ls -r minio/tpch/hudi/lineitem
[2023-12-30 15:50:27 CET] 2.6KiB STANDARD .hoodie/20231230154852996.commit
[2023-12-30 15:49:01 CET]     0B STANDARD .hoodie/20231230154852996.commit.requested
[2023-12-30 15:49:01 CET]     0B STANDARD .hoodie/20231230154852996.inflight
[2023-12-30 15:50:27 CET] 1.6KiB STANDARD .hoodie/hoodie.properties
[2023-12-30 15:49:01 CET]    96B STANDARD .hoodie_partition_metadata
[2023-12-30 15:50:27 CET] 348MiB STANDARD 903b5609-0725-4124-98d2-ce8aa9e34ba6-0_0-4-0_20231230154852996.parquet
```

For Iceberg:

```bash
$ mcli ls -r minio/tpch/iceberg/lineitem
284MiB data/20231230_142310_00013_wm2mp-dd02b9ed-a7f8-4e1f-8866-ca5c613fa3ab.parquet
3.3KiB metadata/00000-b771789e-2644-4eed-a395-8b2ca2bde7a2.metadata.json
7.8KiB metadata/87b968c5-7be2-4d18-be6b-7c1fe484183f-m0.avro
4.2KiB metadata/snap-7849754039344149129-1-87b968c5-7be2-4d18-be6b-7c1fe484183f.avro
```

For Delta Lake:

```bash
$ mcli ls -r minio/tpch/delta/lineitem
284MiB 20231230_142403_00014_wm2mp_d48ae366-e8df-4eb4-8104-de253aebff46
3.1KiB _delta_log/00000000000000000000.json
```

The row group boundaries between Hive, Iceberg, and Delta Lake are not exactly the same, but similar enough, as determined by the `parquet_row_group_sizes` tool (see @tools).

For Hive:

```bash
$ parquet_row_group_sizes 20231230_134731_00018_9tjiz_7ff34205-31f3-4202-973c-6397ccec9ac9
0: 125529834 B compressed / 322373338 B uncompressed
1: 125716341 B compressed / 322832134 B uncompressed
2: 47070114 B compressed / 119097686 B uncompressed
```

For Iceberg:

```bash
$ parquet_row_group_sizes 20231230_142310_00013_wm2mp-dd02b9ed-a7f8-4e1f-8866-ca5c613fa3ab.parquet
0: 125700190 B compressed / 322812651 B uncompressed
1: 125715802 B compressed / 322831450 B uncompressed
2: 46895278 B compressed / 118659434 B uncompressed
```

For Delta Lake:

```bash
$ parquet_row_group_sizes 20231230_142403_00014_wm2mp_d48ae366-e8df-4eb4-8104-de253aebff46
0: 125698970 B compressed / 322812601 B uncompressed
1: 125714559 B compressed / 322831467 B uncompressed
2: 46894530 B compressed / 118659151 B uncompressed
```

We then ran the benchmark again. The results are shown in @tpch_query1_sf2_refined_results.

#figure(
  table(
    columns: 4,
    [*Table Format*], [*N*], [*Mean Time [ms]*], [*Standard Deviation [ms]*],
    [Hive], [30], [1310], [27.3],
    [Hudi], [30], [2211], [8.01],
    [Iceberg], [30], [1349], [23.4],
    [Delta Lake], [30], [1293], [27.5],
  ),
  caption: flex.flex-caption(
    [Refined benchmark results of TPC-H query 1 with a scale factor of 2],
    [TPC-H Q1 SF=2: Refined results],
  ),
) <tpch_query1_sf2_refined_results>

Hive, Iceberg and Delta Lake now have comparable query runtimes. Hudi is a bit of an outlier here, as it was created by a different piece of software, more metadata, and a non-configurable split size, resulting in a single task having to process the entire file.


=== Scale Factor 100 <tpch_query1_sf100>

As a final test, we repeated the query with a scale factor of 100. The number of records in the `lineitem` table was 600,037,902, and the size of the bucket was 15 GiB for Hive, Iceberg, and Delta Lake, and 18 GiB for Hudi.

#figure(
  table(
    columns: 3,
    [*Table Format*], [*Mean Time [ms]*], [*Standard Deviation [ms]*],
    [Hive], [38643], [128],
    [Hudi], [38866], [93.4],
    [Iceberg], [38493], [96.1],
    [Delta Lake], [38462], [109],
  ),
  caption: flex.flex-caption(
    [Benchmark results of TPC-H query 1 with a scale factor of 100],
    [TPC-H Q1 SF=100: Results],
  ),
) <tpch_query1_sf100_results>

The mean query runtime in @tpch_query1_sf100_results is almost exactly the same for all table formats. Hive, Iceberg, and Delta Lake all use 138 splits and process an average of 1,714,394.01 rows per task. Hudi, due to its fixed split size and relatively large files, uses only 17 splits and processes an average of 35,296,347.18 rows per task.

Despite the significant differences in split processing, the total CPU time is almost exactly the same for all table formats: 55.67s with a total scheduled time of 9.92m, meaning that most of the time was spent waiting for data.  According to @tpch_sar_cpu_fig the query was neither CPU-bound nor I/O-bound (given the low system utilization), but there was a clear plateau in network I/O performance as shown in @tpch_sar_network_fig. The MinIO node did not show any significant resource exhaustion patterns, making the source of the bottleneck unclear.

#figure(
  image("/_img/tpch/sar_delta_cpu.svg"),
  caption: flex.flex-caption(
    [CPU utilization of the worker broken down by %usr and %sys during TPC-H query 1 with a scale factor of 100],
    [TPC-H Q1 SF=100: CPU utilization],
  ),
) <tpch_sar_cpu_fig>

#figure(
  image("/_img/tpch/sar_delta_network.svg"),
  caption: flex.flex-caption(
    [Network I/O utilization of the worker during TPC-H query 1 with a scale factor of 100],
    [TPC-H Q1 SF=100: Network I/O utilization],
  ),
) <tpch_sar_network_fig>


=== Conclusions

Developing a fair benchmark that captures all aspects of each format is challenging. Despite conceptual similarities between table formats, there are many differences to consider. We are confident that by adjusting the storage format, compression codec, number of writers, split size, and table-specific features (such as disabling the Hudi metadata table), we have found a common ground that allows for further comparisons.

The TPC-H Q1 performance is mostly the same for all formats except Hudi because it has to process more data differently than the other formats. Especially at higher scale factors, we found that the choice of table formats becomes secondary when we hit I/O bottlenecks. Comparing table formats in this way does not seem promising; we need to eliminate the overhead of data transfers. In addition, for many TPC-H queries, query performance is more indicative of query engine performance than table format performance. Therefore, in the next section, we perform load tests using a minimal dataset and table scans.
